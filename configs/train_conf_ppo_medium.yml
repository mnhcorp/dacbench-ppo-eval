experiment:
    n_cores:                        1
    log_level:                      1
    n_episodes:                     100000
    n_steps:                        500000
    save_interval:                  2000
    eval_interval:                  2000
    eval_n_episodes:                50    

bench:
    name:                           "Theory"
    discrete_action:                True
    action_choices:                 [1,13,25,37]
    problem:                        "LeadingOnes"    
    instance_set_path:              "lo_rls_100_random.csv"
    observation_description:        "n,f(x)"
    reward_choice:                  "imp_minus_evals"
    seed:                           0
    
eval_env:
    reward_choice:                  "minus_evals"
    
agent:
    name:                           "ppo"
    epsilon:                        0.2
    begin_learning_after:           10000
    batch_size:                     4096
    gamma:                          0.9998
    gae_lambda:                     0.8
      #learning_rate:                  0.0006

